{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# Data exploration\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes=pd.read_csv(\"../input/diabetes.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Diabetes data set dimensions : {}\".format(diabetes.shape))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes.groupby(\"Outcome\").size()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"diabetes.isnull().sum()\ndiabetes.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(diabetes.BloodPressure)\nplt.xlabel(\"Blood Pressure Level\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#looks like there are people with 0 blood pressure levels..identify them as those could be wrong observations(zero BP for a living person??)\ndiabetes[diabetes.BloodPressure==0].Outcome.value_counts()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.hist(diabetes.Glucose)\nplt.xlabel(\"Glucose Level\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# zero glucose levels????identify those as they could be wrong observations...\ndiabetes[diabetes.Glucose==0].Outcome.value_counts()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Skin fold thickness. For normal people skin fold thickness can’t be less than 10 mm better yet zero. Identify those.\nplt.hist(diabetes.SkinThickness)\nplt.xlabel(\"Skin Thickness\")\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# zero skin thickness levels????identify those as they could be wrong observations...\ndiabetes[diabetes.SkinThickness==0].Outcome.value_counts()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BMI. Identify any anomalies\nplt.hist(diabetes.BMI)\nplt.xlabel(\"BMI\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#BMI; should NOT be 0 for a living person\ndiabetes[diabetes.BMI==0].Outcome.value_counts()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Insulin. anomaly identification\nplt.hist(diabetes.Insulin)\nplt.xlabel(\"Insulin\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Looks like some zeros for insulin as well. Identify those\ndiabetes[diabetes.Insulin==0].Outcome.value_counts()   ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Here are several ways to handle invalid data values :\n**\nIgnore/remove these cases : This is not actually possible in most cases because that would mean losing valuable information. And in this case “skin thickness” and “insulin” columns means have a lot of invalid points. But it might work for “BMI”, “glucose ”and “blood pressure” data points.\nPut average/mean values : This might work for some data sets, but in our case putting a mean value to the blood pressure column would send a wrong signal to the model.\nAvoid using features : It is possible to not use the features with a lot of invalid values for the model. This may work for “skin thickness” but its hard to predict that."},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove rows for which the “BloodPressure”, “BMI” and “Glucose” are zero.\ndiabetes_mod = diabetes[(diabetes.BloodPressure != 0) & (diabetes.BMI != 0) & (diabetes.Glucose != 0)]\nprint(diabetes_mod.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feature_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\nX = diabetes_mod[feature_names]\ny = diabetes_mod.Outcome","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let us start with model selection first here. Since this is a classification problem, import all those relevant models/.\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us use default parameters and initialize the models accordingly\nmodels = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('SVC', SVC()))\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('GB', GradientBoostingClassifier()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#avoid training and testing on the same data as the goal of model is to predict out of sample data. Henc, follow:\n1. Train/Test split\n2. K-Fold Cross validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train/test using stratify \nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify = diabetes_mod.Outcome, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_train.value_counts(normalize=True))\nprint(y_test.value_counts(normalize=True))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**# as you can observe...proportion of 0 and 1 of outcome remains the same even after split due to the fact that we used:\nstratify = diabetes_mod.Outcome during train_test_split**"},{"metadata":{"trusted":true},"cell_type":"code","source":"names = []\nscores = []\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    scores.append(accuracy_score(y_test, y_pred))\n    names.append(name)\ntr_split = pd.DataFrame({'Name': names, 'Score': scores})\nprint(tr_split)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort them by descending order of accuracy score\ntr_split.sort_values(by=\"Score\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# K fold cross validation ---perfect way of measuring accuracy of models\nfrom sklearn.model_selection import KFold\nnames = []\nscores = []\nfor name, model in models:\n    \n    kfold = KFold(n_splits=10, random_state=10) \n    score = cross_val_score(model, X, y, cv=kfold, scoring='accuracy').mean()\n    \n    \n    names.append(name)\n    scores.append(score)\nkf_cross_val = pd.DataFrame({'Name': names, 'Score': scores})\nprint(kf_cross_val)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# sort them by descending order of accuracy score\nkf_cross_val.sort_values(by=\"Score\",ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.bar(kf_cross_val.Name,kf_cross_val.Score)\nplt.xlabel(\"Model name\")\nplt.xlabel(\"cross validation accuracy score\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Looks like Gradient Boosting, Logistic regression performed better than others it seems**\n\n***Let us explore more into feature engineeing and hyper parameter tuning to achieve more accuracy*****"},{"metadata":{},"cell_type":"markdown","source":"# Feature selection/engineering: let us apply that on logistic regression: Methods that we can use on this:\n\n1. Univariate feature selection:  Selecting features that have the strongest relationship with  output variable\n2. Recursive Feature elimination: works by recursively removing attributes and building a model on those attributes that remain. It uses the model accuracy to identify which attributes (and combination of attributes) contribute the most to predicting the target attribute.\n\n3.  PCA: Principal component analysis\n\n4. Feature importance: ensemble models like Random Forest and Extra Trees can be used to estimate the importance of features\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import StratifiedKFold\nstrat_k_fold=StratifiedKFold(n_splits=10)\nlogreg_model = LogisticRegression()\nrfecv=RFECV(logreg_model,step=1,cv=strat_k_fold,scoring=\"accuracy\")\nrfecv.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print the feature indices being selected\nprint(rfecv.get_support(indices=True))\nprint(rfecv.get_support(indices=False))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rfecv.grid_scores_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure()\nplt.title('Logistic Regression CV score vs No of Features')\nplt.xlabel(\"Number of features selected\")\nplt.ylabel(\"Cross validation score\")\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of best features: \",rfecv.n_features_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Features and their ranking\nrfecv.ranking_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#feature indices with best ranking. In other words, identify those feature indices first, followed by their names\nbest_feature_indices=np.where(rfecv.ranking_==1)\nbest_feature_names=X.columns[best_feature_indices]\nprint(\"Best feature names: \",best_feature_names)  #features most suitable for predicting the outcome variable","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"****Let us do a comparison of the model with original features vs new best features****"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_new=diabetes_mod[best_feature_names]\ninitial_score = cross_val_score(logreg_model, X, y, cv=strat_k_fold, scoring='accuracy').mean()\nprint(\"Initial accuracy : {} \".format(initial_score))\n\nfe_score = cross_val_score(logreg_model, X_new, y, cv=strat_k_fold, scoring='accuracy').mean()\nprint(\"Accuracy after Feature Selection : {} \".format(fe_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## there seems to be a slight increase in accuracy after selecting the best features. All features: 0.7764 & best features only: 0.78058"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Let us apply the same concent to next model :gradient boosting as well\ngb_model = GradientBoostingClassifier()\nrfecv_gb=RFECV(gb_model,step=1,cv=strat_k_fold,scoring=\"accuracy\")\nrfecv_gb.fit(X,y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of Best selected features: \",rfecv_gb.n_features_)\nprint(\"Best Features' ranks: \",rfecv_gb.ranking_)\nprint(\"Best Features' indices: \",rfecv_gb.get_support(indices=True))\nprint(\"Accuracy scores of features selected: \",rfecv_gb.grid_scores_)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks like it selected 6 features and the score says:0.78197606 for 6 features together\n "},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_best_features=X.columns[rfecv_gb.get_support(indices=True)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gb_best_features","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_gb=diabetes_mod[gb_best_features]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Gradeint Boost -accuracy with all features\ninitial_score = cross_val_score(gb_model, X, y, cv=strat_k_fold, scoring='accuracy').mean()\nprint(\"Initial accuracy : {} \".format(initial_score))\n\n# Gradeint Boost -accuracy 6 best features\nafter_score = cross_val_score(gb_model, X_gb, y, cv=strat_k_fold, scoring='accuracy').mean()\nprint(\"Post accuracy : {} \".format(after_score))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## It seems like Gradient Boost accuracy is slightly better than Logistic regression model after recursive feature elimination process. So, let's go for it. Now, let us do hyperparameter tuning to optimize the model."},{"metadata":{"trusted":true},"cell_type":"code","source":"#import gridsearch model\nfrom sklearn.model_selection import GridSearchCV","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"params= {'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200)}\ngrid = GridSearchCV(estimator = gb_model,param_grid = params, scoring='accuracy',n_jobs=-1,iid=False,cv=strat_k_fold)\ngrid.fit(X_gb,y)\ngrid.best_estimator_,grid.best_params_,grid.best_score_","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# as we can observe, there is a very slight increase in accuracy score. not much though."},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}